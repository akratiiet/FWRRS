{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "783a0b5b-4618-46e9-9fc4-3627f01cb4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import of packages\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random  \n",
    "import numpy as np\n",
    "import sys\n",
    "from  fractions import Fraction\n",
    "import timeit\n",
    "from time import sleep\n",
    "from collections import defaultdict\n",
    "import heapq as heap\n",
    "from tqdm import tqdm\n",
    "from community import community_louvain\n",
    "import collections \n",
    "import matplotlib.cm as cm\n",
    "import statistics\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from itertools import chain, combinations\n",
    "from itertools import islice\n",
    "from collections import Counter, defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from joblib import Parallel, delayed\n",
    "import csv\n",
    "from pprint import pprint\n",
    "import math\n",
    "from operator import itemgetter\n",
    "import itertools\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from IPython.core.magic import register_line_magic\n",
    "from IPython.display import HTML, display\n",
    "import json\n",
    "import powerlaw\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c0a2da1-a2e4-4b53-9b32-2c812575ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_communities(part,G): # returns the number of communities in G given the partition part\n",
    "    communities = []\n",
    "    for i in G.nodes():\n",
    "        if part[i] not in communities:\n",
    "            communities.append(part[i])\n",
    "    return len(communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e25a7797-9575-43a8-9319-ee5487abd6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# genarates a WRR of graph G, starting from any node in S.\n",
    "# takes as input the negative seed nodes S_N, the precomputed paths through which S_N might propaget its information (used to boost the performance)\n",
    "def construct_WRR(G,S,p,S_N,paths):\n",
    "    max_depth= float('inf') \n",
    "    WRR=set()\n",
    "    A,s,m,original_node,copies,ancestors_u,descendants_u,contribution,other=[],{},{},{},defaultdict(lambda: 0),defaultdict(lambda: set()),defaultdict(lambda: set()),{},defaultdict(lambda: 0)\n",
    "    r= random.choices(list(S),weights=p,k=1)[0]\n",
    "    S_N_WRR=set()\n",
    "    WRR.add(r);A.append((r,0));original_node[r]=r\n",
    "    prev_v=defaultdict(lambda: [])\n",
    "    while A:\n",
    "        v,l = A.pop(0)\n",
    "        s[v],m[v]=l,0\n",
    "        if v==r: m[v]=1\n",
    "        else: # compute mv\n",
    "            v_original=original_node[v]\n",
    "            c=prev_v[v]\n",
    "            m[v]+=G[v_original][original_node[c]][\"negative\"]*m[c] \n",
    "        \n",
    "        \n",
    "        if original_node[v] not in S_N:\n",
    "            if l>=max_depth: continue\n",
    "                \n",
    "            parents=list(G.pred[original_node[v]].keys())\n",
    "            connections=[u for u in parents if random.random()<=G[u][original_node[v]][\"negative\"]]\n",
    "        \n",
    "            for u in connections:\n",
    "                if u in WRR:\n",
    "                    if u in descendants_u[v]: continue\n",
    "                    if  other[u]>9:continue\n",
    "                    if not any([set(list(descendants_u[v])+[v]).issubset(x) for x in paths[r]]):other[u]+=1\n",
    "                    #add a copy of the node\n",
    "                    copies[u]+=1\n",
    "                    original_node[str(u)+\"copy\"+str(copies[u])]=u\n",
    "                    descendants_u[str(u)+\"copy\"+str(copies[u])].update(list(descendants_u[v])+[v])\n",
    "                    A.append((str(u)+\"copy\"+str(copies[u]),s[v]+1))\n",
    "                    prev_v[str(u)+\"copy\"+str(copies[u])]=v\n",
    "                else:\n",
    "                    #WRR.add_edge(u,v)\n",
    "                    WRR.add(u)\n",
    "                    descendants_u[u].update(list(descendants_u[v])+[v])\n",
    "                    A.append((u,s[v]+1))   \n",
    "                    original_node[u]= u\n",
    "                    prev_v[u]=v   \n",
    "        else:\n",
    "            S_N_WRR.add(v) \n",
    "            max_depth= l\n",
    "                \n",
    "        for node in descendants_u[v]:\n",
    "            ancestors_u[node].update([v])\n",
    "                \n",
    "    prob_inf=1-np.prod([1-m[s_n] for s_n in S_N_WRR])     \n",
    "    if prob_inf!=0:\n",
    "        max_prob=max([m[s_n] for s_n in S_N_WRR])\n",
    "        for u in WRR:\n",
    "            if not str(u).isnumeric() or m[u]< max_prob or u in S_N_WRR: continue\n",
    "            contribution[u]=1\n",
    "        return True,r,1,WRR,contribution,original_node,ancestors_u,s,m,S_N_WRR,copies \n",
    "    return False,r,0,WRR,None,original_node,ancestors_u,s,m,None,None\n",
    "\n",
    "# finds set with maximum blocking power. Used in the loop to generate WRR, required for the stopping criterion\n",
    "def  Max_Coverage_WRS(RRs,roots,k,G,S_N,S,keep,avg_deg):\n",
    "    S_k=set()\n",
    "    need_covering=[x for x,r in enumerate(RRs) ]\n",
    "    num_RRs=len(need_covering)\n",
    "    count_roots_total=Counter(roots.values())\n",
    "    coverage={v:0 for v in G.nodes()}\n",
    "    for j in RRs.keys():\n",
    "        for  v in RRs[j]:\n",
    "            coverage[v]+=1/count_roots_total[roots[j]]\n",
    "    Cov_S_k=0        \n",
    "    while len(S_k)< k:\n",
    "        u = max(coverage,key=coverage.get)\n",
    "        for j in [j for j in need_covering if u in RRs[j]]:\n",
    "            for  v in set(RRs[j]):\n",
    "                coverage[v]-=1/count_roots_total[roots[j]]\n",
    "        need_covering=set(need_covering)-set([j for j in need_covering if u in RRs[j]])\n",
    "        S_k.add(u)\n",
    "        Cov_S_k+= coverage[u]\n",
    "    return S_k,Cov_S_k\n",
    "\n",
    "def construct_WFR(G,S_N):\n",
    "    A,S=[],set()\n",
    "    r= \"new\"\n",
    "    A.append(r)\n",
    "    first=True\n",
    "    original={r:r}\n",
    "    path={r:[]}\n",
    "    while A:\n",
    "        v = A.pop(0)\n",
    "        childreen=iter(G[original[v]])\n",
    "        connections=[u for u in childreen if random.random()<=G[original[v]][u][\"negative\"]]\n",
    "        \n",
    "        for u in connections:\n",
    "            if (u not in S_N or first)  :\n",
    "                if u not in S:\n",
    "                    path[u]=path[v]+[v]\n",
    "                    A.append(u)\n",
    "                    S.add(u)\n",
    "                    original[u]=u \n",
    "        first=False                 \n",
    "    return S,path\n",
    "\n",
    "\n",
    "# genarates a WFR of graph G, starting from any node in S.\n",
    "# takes as input the negative seed nodes S_N, and a threshold, the minimum probability of infection required to consider a path\n",
    "# used to compute which nodes might get infected\n",
    "def WFRS(G,threshold,S_N):\n",
    "    f=Fraction(threshold).limit_denominator()\n",
    "    denom,num=f.denominator,f.numerator\n",
    "    S=set()\n",
    "    probs=defaultdict(lambda: 0)\n",
    "    count=0\n",
    "    paths=defaultdict(lambda: [])\n",
    "    for i in range(1000):\n",
    "        S_new,path=construct_WFR(G,S_N)\n",
    "        S.update(S_new)\n",
    "        count+=1\n",
    "        for s in S_new: probs[s]+=1; paths[s].append(path[s])\n",
    "        if len(S)==len(G.nodes()): break\n",
    "    for s in S: probs[s]*=1/count\n",
    "    S=[s for s in S if probs[s]>=0.01]\n",
    "    return S,probs,paths\n",
    "\n",
    "def ipsilon_WRS(e,d):\n",
    "    return ((2+Decimal(2/3)*e)*((1/d).ln())*(1/(e**2)))\n",
    "\n",
    "\n",
    "def D_SSA_WRS(G,S,S_N,k,e,d,max_depth,probs,avg_deg,dis,paths):\n",
    "    \n",
    "    exp_inf=np.ceil(np.sum([probs[x] for x in S]))\n",
    "    S_orig=S.copy()\n",
    "    n=len(S);d=Decimal(1/n);e=Decimal(e)   \n",
    "    choose=(Decimal(math.factorial(n))/(Decimal(math.factorial(n-k))*Decimal(math.factorial(k))))\n",
    "    d_prime=d/6/choose\n",
    "\n",
    "    N_max=8*(Decimal(1-1/math.e)/(2+2*e/3))*(ipsilon_WRS(e,d_prime))*Decimal(n/k)\n",
    "    t_max=math.ceil((2*N_max/(ipsilon_WRS(e,d/3))).log10()/Decimal(2).log10())\n",
    "    t=1\n",
    "    \n",
    "    Lambda=round(ipsilon_WRS(e,d/(3*t_max)))\n",
    "    Lambda_1=1+(1+e)*ipsilon_WRS(e,d/(3*t_max))\n",
    "    \n",
    "    \n",
    "    keep,prob_inf_r,contributions,WRRs,original_nodes,RRs,roots,S_N_u,s,m,S_N_WRR,copies={},{},{},{},{},{},{},{},{},{},{},{}\n",
    "   \n",
    "    p=[dis[x] for x in S]\n",
    "    i=0\n",
    "    \n",
    "    while len([x for x,v in keep.items() ])< Lambda*2**(t-1):\n",
    "        keep[i],roots[i],prob_inf_r[i],WRR,contributions[i],original_nodes[i],S_N_u[i],s[i],m[i],S_N_WRR[i],copies[i]=construct_WRR(G,S,p,S_N,paths)\n",
    "        WRRs[i]=[e for e in WRR].copy()\n",
    "        RRs[i]=[n for n in WRR].copy()\n",
    "        i+=1\n",
    "        \n",
    "    RRs_kept=[x for x in keep ]\n",
    "    RRs_kept_true=[x for x in keep if keep[x]]\n",
    "    \n",
    "    while len(RRs_kept)< N_max:\n",
    "        RRs_kept=[x for x in keep]\n",
    "        RRs_kept_true=[x for x in keep if keep[x]]\n",
    "        keep_t,prob_inf_r_t,contributions_t,WRRs_t,original_nodes_t,RRs_t,roots_t,S_N_u_t,s_t,m_t,S_N_WRR_t,copies_t={},{},{},{},{},{},{},{},{},{},{},{}\n",
    "        i=len(keep) \n",
    "        while len([x for x,v in keep_t.items()])< Lambda*2**(t-1):\n",
    "            keep_t[i],roots_t[i],prob_inf_r_t[i],WRR_t,contributions_t[i],original_nodes_t[i],S_N_u_t[i],s_t[i],m_t[i],S_N_WRR_t[i],copies_t[i]=construct_WRR(G,S,p,S_N,paths)\n",
    "            \n",
    "            WRRs_t[i]=[e for e in WRR_t].copy()\n",
    "            RRs_t[i]=[n for n in WRR_t].copy()\n",
    "            i+=1\n",
    "          \n",
    "        RRs_t_kept_true=[x for x in keep_t if keep_t[x]]\n",
    "        RRs_t_kept=[x for x in keep_t]\n",
    "        S_k,I = Max_Coverage_WRS(RRs,roots,k,G,S_N,S,keep,avg_deg) \n",
    "        \n",
    "        count_roots_total_t=Counter(roots_t.values())\n",
    "        count_roots_total=Counter(roots.values())\n",
    "        Cov_S_k_t =np.sum([1/count_roots_total_t[roots_t[i]] if ( any([j in RRs_t[i] for j in S_k]) ) else 0 for i,v in keep_t.items()]) \n",
    "        if Cov_S_k_t*len(RRs_t)/n>= Lambda_1:\n",
    "            I_t= Cov_S_k_t\n",
    "            e1=Decimal((I/I_t)-1)\n",
    "            e2=e*np.sqrt(Decimal(n*(1+e))/Decimal(2**(t-1)*I_t))\n",
    "            e3=e*np.sqrt(Decimal(n*(1+e)*(Decimal(1-1/math.e)-e))/Decimal((1+e/3)*2**(t-1)*Decimal(I_t)))\n",
    "            e_t=(e1+e2+e1*e2)*(Decimal(1-1/math.e)-e)+Decimal(1-1/math.e)*e3\n",
    "            \n",
    "            if e_t<=e :\n",
    "                Cov_S_k_s=np.sum([1/count_roots_total[roots[i]] if ( any([j in RRs[i] for j in S_N]) ) else 0 for i,v in keep.items()])\n",
    "                if Cov_S_k_s*len(RRs)/n>=Lambda_1: \n",
    "                    return keep,contributions,WRRs,original_nodes,roots,RRs,S_N_u,s,m,S_N_WRR,copies,prob_inf_r,e_t \n",
    "                Cov_S_t_k_s=np.sum([1/count_roots_total_t[roots_t[i]] if ( any([j in RRs_t[i] for j in S_N]) ) else 0 for i,v in keep_t.items()])\n",
    "                \n",
    "                if Cov_S_k_s*len(RRs)/n+Cov_S_t_k_s*len(RRs_t)/n>=Lambda_1:\n",
    "                    contributions.update(contributions_t)\n",
    "                    WRRs.update(WRRs_t)\n",
    "                    original_nodes.update(original_nodes_t)\n",
    "                    RRs.update(RRs_t)\n",
    "\n",
    "                    roots.update(roots_t)\n",
    "                    S_N_u.update(S_N_u_t)\n",
    "                    s.update(s_t)\n",
    "                    m.update(m_t)\n",
    "                    copies.update(copies_t)\n",
    "                    keep.update(keep_t)\n",
    "                    S_N_WRR.update(S_N_WRR_t)\n",
    "                    prob_inf_r.update(prob_inf_r_t)\n",
    "                    return keep,contributions,WRRs,original_nodes,roots,RRs,S_N_u,s,m,S_N_WRR,copies,prob_inf_r,e_t\n",
    "        \n",
    "        t+=1\n",
    "        \n",
    "        contributions.update(contributions_t)\n",
    "        WRRs.update(WRRs_t)\n",
    "        original_nodes.update(original_nodes_t)\n",
    "        RRs.update(RRs_t)\n",
    "       \n",
    "        roots.update(roots_t)\n",
    "        S_N_u.update(S_N_u_t)\n",
    "        s.update(s_t)\n",
    "        m.update(m_t)\n",
    "        copies.update(copies_t)\n",
    "        keep.update(keep_t)\n",
    "        S_N_WRR.update(S_N_WRR_t)\n",
    "        prob_inf_r.update(prob_inf_r_t)\n",
    "    return keep,contributions,WRRs,original_nodes,roots,RRs,S_N_u,s,m,S_N_WRR,copies,prob_inf_r,e_t\n",
    "\n",
    "\n",
    "\n",
    "#this is the WRRS method proposed in the paper\n",
    "def WRS_method(G,S_N,k,partition,threshold,min_inf):\n",
    "    avg_deg=sum([G.out_degree(x) for x in G.nodes()])/len(G.nodes())\n",
    "    probs={}\n",
    "    G_extended=G.copy()\n",
    "    G_extended.add_edges_from([(\"new\",x) for x in S_N])\n",
    "    for x in S_N:G_extended[\"new\"][x][\"negative\"]=1\n",
    "    S,probs,paths=WFRS(G_extended,threshold,S_N)\n",
    "    paths_blocked=defaultdict(lambda: 0)\n",
    "    S=list(set(S)-set(S_N+[\"new\"]))\n",
    "    d={x:probs[x] for x in S}\n",
    "    paths={x:paths[x] for x in S}\n",
    "    \n",
    "    S_P=[]\n",
    "    e=0.1\n",
    "    keep,contributions,WRRs,original_nodes,roots_all,RRs,ancestors_u,s,m,S_N_WRR,copies,prob_inf_r,e_new= D_SSA_WRS(G,S,S_N,k,e,1/len(G),partition,probs,avg_deg,d,paths)\n",
    "    del WRRs, ancestors_u,S_N_WRR,copies,prob_inf_r,e_new\n",
    "    \n",
    "    WRRs_to_keep=[i for i,b in enumerate(keep.values()) if b]\n",
    "    WRRs_to_remove=[i for i,b in enumerate(keep.values()) if not b]\n",
    "    del keep\n",
    "    \n",
    "    num_com=get_communities(partition,G)\n",
    "    roots={key:roots_all[key] for key in WRRs_to_keep}\n",
    "    count_roots=Counter(roots.values())\n",
    "    count_roots_total=Counter(roots_all.values())\n",
    "    \n",
    "    \n",
    "    RRs_copies=RRs.copy()\n",
    "    prob_infections={x:count_roots[x]/count_roots_total[x] for x in count_roots.keys()}\n",
    "    current_prob_infections=defaultdict(lambda: 0)\n",
    "    inform_power=defaultdict(lambda: 0)\n",
    "      \n",
    "    initial_infections={}\n",
    "    \n",
    "    for x in count_roots.keys():\n",
    "        current_prob_infections[x]=count_roots[x]/count_roots_total[x]\n",
    "        if partition[x] not in initial_infections.keys():initial_infections[partition[x]]=0\n",
    "        initial_infections[partition[x]]+=prob_infections[x]\n",
    "    \n",
    "    expected_inf=initial_infections.copy()\n",
    "    communities_to_save=[x for x in initial_infections.keys() if initial_infections[x]>0]\n",
    "    Gcc = sorted(nx.connected_components(G.to_undirected()), key=len, reverse=True)\n",
    "    G_sub = G.subgraph(Gcc[0])\n",
    "    \n",
    "    com_g_sub=set([partition[x] for x in G_sub.nodes()])\n",
    "    \n",
    "    min_help_c=communities_to_save.copy()\n",
    "    candidates=list(G.nodes())\n",
    "    maximin_value=0\n",
    "    first=True\n",
    "    \n",
    "    \n",
    "    maximin_c= {c:(initial_infections[c]-expected_inf[c])/initial_infections[c] for c in communities_to_save }\n",
    "    total_inform=defaultdict(lambda: 0)\n",
    "    while len(S_P)< k:\n",
    "        max_c=[c for c in communities_to_save if expected_inf[c]==max(expected_inf.values())][0]\n",
    "        no_help=[c for c in communities_to_save if (initial_infections[c]-expected_inf[c])/initial_infections[c]==0]\n",
    "        need_saving=[c for c in communities_to_save if (initial_infections[c]-expected_inf[c])/initial_infections[c]<=maximin_value*(1+e)+len(no_help)/len(communities_to_save)*e]\n",
    "        \n",
    "        if first:\n",
    "            inf_u={x:expected_inf.copy() for x in candidates}\n",
    "            candidates_next=set()\n",
    "            for j in WRRs_to_keep:\n",
    "                for  key in contributions[j].keys():\n",
    "                    candidates_next.add(key)\n",
    "                    inf_u[key][partition[roots[j]]]-=1/count_roots_total[roots[j]]\n",
    "                    total_inform[key]+=1/count_roots_total[roots[j]]\n",
    "             \n",
    "            for j in WRRs_to_remove:\n",
    "                for  key in RRs[j]:\n",
    "                    if not str(key).isnumeric(): continue\n",
    "                    candidates_next.add(key)\n",
    "                    total_inform[key]+=1/count_roots_total[roots_all[j]]\n",
    "            candidates=candidates_next\n",
    "            \n",
    "                    \n",
    "        maximin_c= {c:(initial_infections[c]-expected_inf[c])/initial_infections[c] for c in communities_to_save }\n",
    "                \n",
    "        maximin_pev= min([(initial_infections[c]-expected_inf[c])/initial_infections[c] for c in communities_to_save ]) \n",
    "        \n",
    "        maximin_u={x: min([(initial_infections[c]-inf_u[x][c])/initial_infections[c] for c in communities_to_save ]) for x in candidates}\n",
    "        max_value=max([maximin_u[x] for x in maximin_u.keys()])\n",
    "        u_maximin=max(maximin_u,key=maximin_u.get)\n",
    "        maximins_u_m={c:(initial_infections[c]-inf_u[u_maximin][c])/initial_infections[c] for c in communities_to_save}\n",
    "        c_s=[k for k,x in maximins_u_m.items() if x==max_value]\n",
    "        c=c_s[0]\n",
    "        max_inf=max([expected_inf[c] for c in communities_to_save])\n",
    "        c_s=[c for c in communities_to_save if expected_inf[c]==max_inf]\n",
    "        c=c_s[0]\n",
    "        exp_total=np.sum([expected_inf[c]  for c in communities_to_save ])\n",
    "        u_s=[key for key, value in maximin_u.items() if  value>= max_value*(1-e*(1-expected_inf[c]/exp_total)) ]\n",
    "        \n",
    "        if len(u_s)>1:\n",
    "        \n",
    "            avg_help=np.mean([(initial_infections[c]-expected_inf[c])/initial_infections[c] for c in communities_to_save ])\n",
    "            \n",
    "            need_saving_2=[c for c in communities_to_save if (initial_infections[c]-expected_inf[c])/initial_infections[c]<=avg_help*(1+e)]\n",
    "            tot={x:np.sum([(expected_inf[c]-inf_u[x][c])  for c in communities_to_save ]) for x in u_s} \n",
    "            \n",
    "            if len(no_help)>0:\n",
    "                inf_need_saving={x:np.sum([(expected_inf[c]-inf_u[x][c])  for c in no_help]) for x in u_s}\n",
    "                exp_no_help=np.sum([expected_inf[c] for c in no_help])\n",
    "                u_s=[x for x in u_s if inf_need_saving[x]>0 or exp_no_help< e*exp_total ]\n",
    "                dis_c=[x for x in no_help if x not in com_g_sub]\n",
    "                if len(dis_c)>0:\n",
    "                    inf_need_saving={x:np.sum([(expected_inf[c]-inf_u[x][c])  for c in dis_c]) for x in u_s}\n",
    "                    u_s=[x for x in u_s if inf_need_saving[x]>0]\n",
    "            else: inf_need_saving={x:1 for x in u_s}  \n",
    "                \n",
    "              \n",
    "            inf_need_saving_2={x:np.sum([(expected_inf[c]-inf_u[x][c])*(avg_help-maximin_c[c])   for c in need_saving_2]) for x in u_s }\n",
    "            \n",
    "\n",
    "            max_saved=max(inf_need_saving_2.values())\n",
    "            exp_i=np.sum([expected_inf[c]  for c in need_saving_2])\n",
    "            init_i=np.sum([initial_infections[c]  for c in need_saving_2])\n",
    "            options_u=[key for key, value in inf_need_saving_2.items() if  value >= (1-e)*max_saved-max(init_i-exp_i,0)*avg_help]\n",
    "            \n",
    "            \n",
    "            inf_need_saving={x:np.sum([max(expected_inf[c]-inf_u[x][c],0) for c in need_saving]) for x in options_u}\n",
    "            max_saved_tot=max([inf_need_saving[x] for x in options_u])\n",
    "            options_u_2=[key for key, value in inf_need_saving.items() if  value >= (1-e)*max_saved_tot-e*max(init_i-exp_i,0) ]\n",
    "            \n",
    "            if max([tot[x] for x in options_u_2])< max(tot.values())*(1-e):\n",
    "                sav_tot={x:np.sum([inf_u[x][c] for c in communities_to_save])  for x in options_u_2}\n",
    "                opts=[x for x,v in sav_tot.items() if v<= min(sav_tot.values())*(1+e)]\n",
    "                sav_u={x:np.sum([current_prob_infections[v] for v in list(G.successors(x))+[x] ]) for x in opts}\n",
    "            else:    \n",
    "                sav_u={x:np.sum([current_prob_infections[v] for v in list(G.successors(x))+[x] ]) for x in options_u_2}\n",
    "            u=max(sav_u,key=sav_u.get)\n",
    "                                                                                                                                  \n",
    "        else: u= u_s[0]\n",
    "            \n",
    "        \n",
    "        \n",
    "        expected_inf=inf_u[u].copy()\n",
    "        helps_u={c:(initial_infections[c]-expected_inf[c])/initial_infections[c] for c in communities_to_save}\n",
    "        min_help=min(helps_u.values())\n",
    "        maximin_value=min_help\n",
    "        S_P.append(u)\n",
    "        \n",
    "        first=False\n",
    "        candidates-=set([u])\n",
    "        to_remove=set()\n",
    "        for j in [j for j in WRRs_to_keep if u in contributions[j] ]:\n",
    "            current_prob_infections[roots[j]]-=1/count_roots_total[roots[j]]\n",
    "            current_prob_infections[roots[j]]=max(0,current_prob_infections[roots[j]]) \n",
    "            for w in set(candidates)-set(contributions[j].keys()):\n",
    "                inf_u[w][partition[roots[j]]]-=1/count_roots_total[roots[j]]\n",
    "            for w in set(candidates)&set(contributions[j].keys()):\n",
    "                total_inform[w]-=1/count_roots_total[roots[j]]\n",
    "        WRRs_to_keep=list(set(WRRs_to_keep)-set([j for j in WRRs_to_keep if u in contributions[j]]))\n",
    "            \n",
    "               \n",
    "    return S_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b7960d0-98bf-48d8-9833-8c5088a0542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to set probability of the edges\n",
    "def set_weight_degree(G,p_n):\n",
    "    for u,v in G.edges():\n",
    "        G[u][v][p_n]= 1/G.in_degree(v)\n",
    "    return G\n",
    "\n",
    "def set_weight_trivalency(G,p_n):\n",
    "    for u,v in G.edges():\n",
    "        G[u][v][p_n]= random.choice([0.1,0.01,0.001])\n",
    "    return G\n",
    "\n",
    "def set_weight_value(G,val,p_n):\n",
    "    for u,v in G.edges():\n",
    "        G[u][v][p_n]= round(val,3)\n",
    "    return G\n",
    "\n",
    "def set_weight_average(G,p_n):\n",
    "    avg=sum([G.out_degree(x) for x in G.nodes()])/len(G.nodes())\n",
    "    for u,v in G.edges():\n",
    "        G[u][v][p_n]= 1/avg\n",
    "    return G\n",
    "\n",
    "def set_weight_comm(G,inter,intra,p_n):\n",
    "    for u,v in G.edges():\n",
    "        if G.nodes[u]['community']== G.nodes[v]['community']:\n",
    "            G[u][v][p_n]= inter\n",
    "        else:\n",
    "            G[u][v][p_n]= intra\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d2a9f33-8ed3-4937-9411-c1248909667f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[950, 280, 978, 646, 353, 274, 44, 391, 803, 625, 88, 688, 309, 412, 902, 727, 622, 711, 626, 181, 77, 317, 163, 520, 133, 676, 748, 427, 549, 988, 304, 679, 52, 225, 377, 852, 351, 883, 430, 197, 862, 456, 737, 419, 126, 917, 34, 93, 998, 618, 70, 990, 838, 510, 999, 389, 818, 316, 481, 546, 712, 958, 761, 434, 757, 65, 318, 293, 143, 4, 184, 582, 403, 463, 261, 657, 339, 796, 884, 234, 611, 431, 69, 249, 313, 485, 522, 121, 684, 266, 776, 794, 514, 357, 22, 321, 277, 756, 360, 152]\n"
     ]
    }
   ],
   "source": [
    "network=nx.erdos_renyi_graph(1000,0.4,directed=True)\n",
    "# the network nodes need to have a lavel commuity defining to which community they belong to\n",
    "partition=community_louvain.best_partition(network.to_undirected())\n",
    "for node in network.nodes():\n",
    "    network.nodes[node]['community']=partition[node]\n",
    "    \n",
    "# the network shall also have allocated to each edge the probability of infection        \n",
    "network=set_weight_degree(network,\"positive\")\n",
    "network=set_weight_degree(network,\"negative\")\n",
    "\n",
    "k_p=100\n",
    "k_n=50\n",
    "S_N=random.choices([v for v in list(network.nodes())],k= k_n)\n",
    "S_P=WRS_method(network,S_N,k_p,partition,0.01,0)\n",
    "print(S_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd7bd74-52c9-4a5f-b538-a15c4205dac4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
